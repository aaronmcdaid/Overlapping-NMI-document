\documentclass[9pt,technote]{IEEEtran}
%\documentclass{article}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{natbib}
\usepackage{url}
\usepackage[]{hyperref}
\usepackage{cleveref}
\usepackage{amsmath,amssymb}
\usepackage{rotating}

\author{Aaron McDaid 2010-03-02}
\title{Mutual Information for overlapping groupings}
\begin{document}

\maketitle


\newcommand{\grouping}{\emph{grouping{}}}

\citet{lancichinetti-2009} state the generic \emph{normalized mutual information}
\begin{equation}
	I_{norm}(X:Y) = \frac { H(X) + H(Y) - H(X,Y) } { \left( H(X) + H(Y) \right ) /2 }   \label{NMI}
\end{equation}
and a normalized \emph{variation of information} 
\begin{equation}
	V'_{norm}(X,Y) = \frac12 \left(  \frac{ H(X|Y) }{ H(X) } + \frac{ H(Y|X) }{ H(Y) } \right)   \label{VOI}
\end{equation}

They proceed to define a measure based on \cref{VOI}, not on \cref{NMI}. In this note we see how to define a measure
strictly based on \cref{NMI}.

Before considering a normalization, it's important to identify the \emph{absolute} value you wish to minimize.
The appropriate measure to minimize is
\begin{equation}
	H(X|Y) + H(Y|X)   \label{CorrectMI}
\end{equation}
and, when normalized \footnote{
	It can be shown that \cref{NMI} is equivalent to this, which I believe to be a more understandable formulation:
 \[
	1 - \frac { H(X|Y) + H(Y|X)  }{ H(X) + H(Y) } 
	\]
} is equivalent to \cref{NMI}
This means the amount of information taken to encode X, given that you can refer to Y as a ``codebook'', plus
the information taken to encode Y, given that you can refer to X as a ``codebook''. If $X$ and $Y$ are similar, then
$H(X|Y)$ and $H(Y|X)$ should be close to zero. Then we can identify a suitable normalization (but see the warning in \cref{sec:AbNormal})

\section{WARNING}
I'm pretty sloppy, as ever, with my terminology. I just take $H(x)$ to mean the number of bits taken to encode $x$. Hence
I tend to think in $\log_2$, the log to the base 2, when technically I suppose it should be $\log_e$. There's lots of other careless things too, no doubt \begin{turn}{90}(-:\end{turn}.

\section{A problem with the nVOI}
\label{sec:Flood}

You can get an ``nVOI'' \eqref{VOI} of at least 0.5 by simply flooding the found grouping with every subset of the population.
Assuming $Y = \mbox{the power set}$ is the \emph{found} grouping, this will make the numerator of the first term
very small. However, this is as much as issue with the definition of $H(X|Y)$ in equation (B.9) of \cite{lancichinetti-2009}.
More on this after revising the basics of how to encode some information.

\section{Basics of encoding}
Given a \grouping, G, made up of $|G|$ groups,
\begin{equation*}
	G = \{ g_1, g_2, \ldots g_{|G|}  \}
\end{equation*}
you can consider G as a matrix of ones and zeros, where each column is vector of $N$ entries recording whether that
node is in that community. A na\"ive encoding scheme is to just set $H(G) = |G| \times N$, which is just one
bit of information for each entry in the matrix. But a more efficient method,
making use of our assumption that community size varies a lot, is to first encode the \emph{size} of the community,
then identify it members. Considering one single community, g,
\begin{equation}
	H(g) = \log_2 N + \log_2 \left( 1/{\binom{N}{|g|}} \right)  \label{OneCommunity}
\end{equation}
where $|g|$ is the number of nodes in group $g$.

This gives us the encoding of an entire (ordered) \grouping,

\[
	H^{*}(G) = \sum_{g \in G} H(g)
\]

But we don't care what order we encode the groups in, so with some handwaving as per the \cite{MOSES}
equivalent grouping argument, we should do
\begin{equation}
	H(G) = \left( \sum_{g \in G} H(g) \right)  - \log_2(|G|!)       \label{SummedG0}
\end{equation}
to get unordered grouping, although this correction mightn't contribute much in many cases. However, I believe that this correction
is important in the case of overlapping communities, as $|G|$ could be very large, especially when
trying to ``cheat'' as per \cref{sec:Flood}.

\section{Encoding with a codebook}
$H(G|F)$ means how many bits does it take to encode G, given that you already have access to F. We'll build it up, one community at a time.
First, the encoding of one community given another (hopefully similar) community, is just the bits taken to encode the difference
between the two communities. I'll define a pseudo-community that is just the nodes that are in $g$, or $f$, but not both, to be $g \Delta f$.
Then
\[
	H(g|f) = H(g \Delta f)
\]
where  $H(g \Delta f)$ is defined as per \cref{OneCommunity} \footnote{
	We probably don't want the complement of the community to be seen as a good match, with low $H(g|f)$, as argued in (B.14) of \cite{lancichinetti-2009}.
	I think this is equivalent to simply requiring that $ |g \Delta f| < N/2 $. And, as they do, define $H(g|f) = H(g)$ if there is no appropriate non-complementary $f$.
}

When you have access to all of $F$, you match $g$ to the best match in $F$, calling if $f^*$. I don't actually use the following
expression, but it's important to illustrate a point.
\begin{equation}
	H(g|F) = \log_2|F| + H(g|f^*)            \label{OneCommFudged}
\end{equation}

The first term, $\log_2|F|$ is left out in (B.9) \cite{lancichinetti-2009}. A term like this is necessary to account for encoding \emph{which} 
found community in the codebook it being used. It is not sufficient to specify which nodes have changed status, one must also identify the community
which is being modified.

So to do the sum in \cref{SummedG0} properly, which requires $H(G,G) = 0$, I'm tempted
\footnote{
	I say ``tempted'' because we're having to fudge something here. Strictly speaking, an efficient encoding,
  which would give us $H(G|G)=0$ would involve allowing each community in $F$ to be selected only once, making
  the first term in \cref{OneCommFudged} a bit smaller, on average, because there would be a smaller set of
	found communities in $F$ to choose from as each g in $G$ in being encoded. So I'm going to cheat a little,
	and fix the ``cheating'' and the $H(G|G)=0$ in another, imperfect, way. The more I think about it though,
	it might even be correct!
}
to do something like this:
\begin{equation}
	H(G|F) = \left( \sum_{g \in G} H(g|f^*) \right)  - \log_2(|G|!) + \log_2(|F|!)       \label{SummedG}
\end{equation}
Here we simply pair up the comms in $G$ and the comms in $F$ by writing them down in two columns and requiring that
each g be paired with one f. Then $\log_2(|F|!)$ is the information needed to present the comms in $F$ in a
suitable order.

Of course, it is unlikely to be the case that $|F|=|G|$, and so we can't really say they're all paired up
with each other. But that doesn't really matter, I think.
This function will do the right thing and be at least as accurate as the existing formulation.

If the \grouping{}s really are identical, then $|F|=|G|$ and the second two terms of \cref{SummedG} will
cancel, and the terms inside the $\sum$ will each be zero as each community will have been lined up
with its best match. If they are very different, then it is important to include these terms as something
is needed to encode which is to match with which.

\section{Bringing it together}
If use use the \emph{normalized mutual information} as per \cref{NMI}, then the log terms
in \cref{SummedG} will cancel out. But they don't cancel in \cref{VOI} and this is a major
reason why the \cref{VOI} can be cheated.

If both are done properly, then I would still have a preference for \cref{NMI}. But maybe then a
case could be made for \cref{VOI}, I don't really know too much about any of this. I have a hunch that, if
\cref{SummedG} is properly used in \cref{VOI} that the two terms are likely to be in the same
ballpark anyway and, also, I suspect that this, corrected, \cref{VOI} would then be similar to \cref{NMI} anyway.


\section{The cult of normalization}
\label{sec:AbNormal}

When looking at the stability of an algorithm, under something like a random renaming of the nodes, it may be more
appropriate to look at the \emph{absolute} (i.e. un-normalized) amount of information. A good community finding algorithm
will find smaller communities \cite{ResLimit} than does modularity maximization \cite{blondel-2008}, and will
also find overlapping communities. Together this will mean that the found partition, $H(F)$, and presumably the ground truth, $H(G)$, will
each contain a lot of information.

Therefore, it might be that the \emph{absolute} amount of information shared between H(G) and H(F) is high. It would be more
appropriate to use (unnormalized) mutual information
\begin{align*}
	I(X:Y) &=  H(X) + H(Y) - H(X,Y)          \\
				&= \frac12 \left( H(X) + H(Y) - H(X|Y) - H(Y|X) \right)
\end{align*}
If this quantity is larger for one algorithm than another, I'd say this is a better way to define it as being more ``stable''.
An an extreme example, a trivial way to get perfect NMI, and an apparently ``stable'' algorithm is
to create a \grouping{} with just one community which takes up the whole graph.

If there is a strong desire for some sort of normalization, then that should be a function of the graph, not of either \grouping.
Perhaps divide by $H(\mathrm{the graph})$?


%\bibliographystyle{IEEEtran}
\bibliographystyle{unsrtnat}
%\bibliographystyle{plain}
\bibliography{community}
\end{document}
