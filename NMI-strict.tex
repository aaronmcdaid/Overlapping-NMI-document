\documentclass[9pt,technote]{IEEEtran}
%\documentclass{article}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{natbib}
\usepackage{url}
\usepackage[]{hyperref}
\usepackage{cleveref}
\usepackage{verbatim}
\usepackage{amsmath,amssymb}
\usepackage{rotating}
\usepackage{tikz}

\author{Aaron McDaid, Derek Greene, Neil Hurley}
\title{Mutual Information for overlapping groupings}
\begin{document}

\maketitle


\newcommand{\grouping}{\emph{grouping{}}}
\newcommand{\lfk}{\cite{lancichinetti-2009} {}}

\begin{abstract}
Given the increasing popularity of algorithms for overlapping clustering, in particular in social network analysis, quantitative measures are needed to measure the accuracy of a method.
Given a set of true clusters, and the set of clusters found by an algorithm, these sets of sets must be compared to see how similar or different the sets are.
A normalized measure is desirable in many contexts, for example assigning a value of 0 where the two sets are totally dissimilar, and 1 where they are identical.

A measure based on normalized mutual information has recently become popular. We demonstrate unintuitive behaviour of this measure, and show how this can be corrected
by using a more conventional normalization. We compare the results to that of other measures, such as the Omega index \cite{collins1988omega}.
\end{abstract}


In a non-overlapping scenario, each node belongs to exactly one cluster. We are looking at overlapping, where a node could belong to many communities, or indeed to no clusters.
Such a set of clusters has been referred to as a \emph{cover} in the literature, and this is the terminology that we will use.

For a good introduction to our problem of comparing covers of overlapping clusters, see \cite{collins1988omega}.
They describe the Rand index, which is defined only for disjoint clusters, and then show how to extend it to overlapping clusters.
Each pair of nodes is considered and the number of clusters in common between the pair is counted. Even if a typical node is in many clusters,
it's likely that a randomly chosen pair of nodes will have zero clusters in common.
%If there are $n$ objects, there are $N=\frac12 N(N-1)$
These counts are calculated for both covers and the Omega index is defined as the proportion of pairs for which the shared-cluster-count is identical,
subject to a correction for chance.

\section{Mutual information}

Meila (which paper to reference?) defined a measure based on mutual information for comparing disjoint clusterings.
\citet{lancichinetti-2009} proposed a measure also based on mutual information, extended for covers.
This measure has become quite popular for comparing community finding algorithms in social network analysis.
It is the measure we are primarily concerned with there, and we will refer to it as LFKNMI named after the authors.

We are proposing to use a different normalization, but first we will define the non-normalized measure.
For full details, look at the final section of \citet{lancichinetti-2009}.

Given two covers, $X$ and $Y$, we must first see how to measure the similarity between a pair of clusters.
$X$ and $Y$ are matrices of cluster membership. There are $n$ objects.
The first cover has $K_X$ clusters, and hence $X$ is an
$n \times K_X$ matrix. $Y$ is an $n \times K_Y$ matrix.

To compare cluster $i$ of the first cover to cluster $j$ of the second cover, we compare the vectors
$X_i$ and $Y_j$. These are vectors of ones and zeroes.

\begin{itemize}
	\item $              { a = \sum_{m=1}^n [X_{i,m} = 0 \wedge  Y_{j,m} = 0] } $
	\item $              { b = \sum_{m=1}^n [X_{i,m} = 0 \wedge  Y_{j,m} = 1] } $
	\item $              { c = \sum_{m=1}^n [X_{i,m} = 1 \wedge  Y_{j,m} = 0] } $
	\item $              { d = \sum_{m=1}^n [X_{i,m} = 1 \wedge  Y_{j,m} = 1] } $
\end{itemize}

If $a+d = n$, and therefore $b=c=0$, then the two vectors are in complete agreement.

The lack of information between two vectors is defined:
\begin{align}
	H(X_i | Y_j) = & {} H(X_i , Y_j) - H(Y_j)  \\
	               = & {} h(a,n) + h(b,n) + h(c,n) + h(d,n)  \\ 
		         & {} - h(b+d,n) - h(a+c,n)
\end{align}
where $h(w,n) = -w \log_2 \frac{w}{n} $

There is an interesting technicality here. Imagine a pair of clusters but where the memberships
have been defined randomly.
There is a possibility that there will be a small amount of mutual information, even in this situation where the two vectors are negatively correlated with each other.
In extremis, if the two vectors are near complements of each other, mutual information will be very high. We wish to override this and define that
there is zero mutual information in this case.
% The mutual information should be positive if and only if the correlation is positive (can we include this?)
This is defined in equation (B.14) of \cite{lancichinetti-2009}.
We also use this restriction in our proposal.

\begin{equation}
	\begin{split}
	H^* & (X_i | Y_j) = \\
	& \left\{
		\begin{split}
			H^*(X_i | Y_j) \; & \mbox{if} \; h(a,n) + h(d,n) \geq h(b,n) + h(c,n) \\
			h(c+d,n)+h(a+b,n) \;  & \mbox{otherwise}
		\end{split}
	\right.
	\end{split}
	\label{eqnNoComplements}
\end{equation}

This allows us to compare vectors $X_i$ and $Y_j$, but we want to compare the entire matrices
$X$ and $Y$ to each other. We will follow the approximation used by \lfk here and
match each vector in $X$ to its best match in $Y$,

\begin{equation}
	H(X_i | Y) = \underset{j \in \{1,\dots K_Y \}}{\min} H(X_i | Y_j)
	\label{eqnBestMatch}
\end{equation}

then summing across all the vectors in $X$,

\begin{equation}
	H(X | Y) = \sum_{i \in \{1,\dots K_X \}} H(X_i | Y)
	\label{eqnSumVectors}
\end{equation}

$H(Y|X)$ is defined in a similar way to $H(X|Y)$, but with the roles reversed.

\section{Useful identities}

\begin{figure}
	\centering
\begin{tikzpicture}
	\draw (-1cm,0) circle(2cm);
	\draw (1cm,0) circle(2cm);
	\draw (0,0cm) node[fill=white] {$I(X:Y)$};
	\draw (1.8cm,0cm) node[fill=white] {$H(Y|X)$};
	\draw (-1.8cm,0cm) node[fill=white] {$H(X|Y)$};
	\path(1cm,0) ++  (60:2.2cm) node[rotate=-25] {$H(Y)$};
	\path(-1cm,0) ++ (120:2.2cm) node[rotate= 25] {$H(X)$};
\end{tikzpicture}
\caption{\label{figVenn} Mutual information and variation of information. The total information $H(X,Y) = H(X|Y) + I(X:Y) + H(Y|X)$. }
\end{figure}


\cref{figVenn} gives us an easy way to remember the following useful identities, which
apply to any mutual information context.

\begin{align*}
	H(X) = & I(X:Y) + H(X|Y) \\
	H(Y) = & I(X:Y) + H(Y|X) \\
	H(X,Y) = & H(X) + H(Y|X) \\
	H(X,Y) = & H(Y) + H(X|Y) \\
	H(X,Y) = & \overbrace{I(X:Y)}^\text{mutual information} + \overbrace{H(X|Y) + H(Y|X)}^\text{variation of information}
\end{align*}

This gives us two definitions for the mutual information, $I(X:Y)$.
In theory, these should be identical, but due to the nature of \cref{eqnBestMatch}
they may be different. Therefore, we will use the average of the two.

\begin{equation}
	I(X:Y) := \frac12 \left[ H(X)-H(X|Y) + H(Y)-H(Y|X) \right]
	\label{eqnAverageOfTwo}
\end{equation}

We are now ready to discuss normalization, contrasting the method of \lfk
with our alternative.

\citet{lancichinetti-2009} define their own normalization of the \emph{variation of information},
\begin{equation}
	VI_{normalized}(X,Y) = \frac12 \left(  \frac{ H(X|Y) }{ H(X) } + \frac{ H(Y|X) }{ H(Y) } \right)   \label{VOI}
\end{equation}
and hence their normalized mutual information is
\begin{equation}
	I_{normalized}(X,Y) = 1 - \frac12 \left(  \frac{ H(X|Y) }{ H(X) } + \frac{ H(Y|X) }{ H(Y) } \right) %  \label{VOI}
\end{equation}

There are of course many ways to normalize a quantity such as the \emph{variation of information}.
Normalization typically involves division by a quantity $c$,
\begin{equation}
	\frac{ H(X|Y) + H(Y|X) } {c(X,Y)} \label{eqnNorms}
\end{equation}
where $c$ is a function of $X$ and $Y$ which is guaranteed to be less than or equal to the numerator.
But LFKNMI does not use a normalization of this standard form, instead using \cref{VOI}.

There is another aspect to the non-standard normalization used in LFKNMI;
they insert an extra normalization factor into their definition of $H(X_i|Y_j)$.
But this is not the root cause of the problems we will describe, hence we will not dwell on it.
Our change is to remove all the normalization steps from their analysis and instead
use a more conventional normalization of the form of \cref{eqnNorms}.

\section{Unintuitive behaviour of LFKNMI}
\label{secUnintuitive}
There are circumstances where LFKNMI overestimates the similarity of two clusters.
We will show how an alternative normalization will fix these problems.

Imagine a cover $X$, and we are comparing it to a cover $Y$. Further, imagine $Y$ has only one
cluster ($K_Y=1$) and this cluster is identical to one of the clusters in $X$.
For large $K_x$, we would expect the normalized mutual information to be quite low.
An intuitive result would be approximately $\frac1{K_X}$.

However, $\mbox{LFKNMI}(X,Y)$ will be at least $0.5$ in cases like this.
This is because $H(Y|X)$ will be zero bits
(the single cluster in $Y$ can be encoded with zero bits because it has a perfect match among the clusters of $X$)
and this will result in a contribution of $0.5$ to the LFKNMI.

The other problematic example involves the power set. There are $n$ objects in total.
A cover involving every subset of the $n$ objects will create $2^n - 1$ clusters; we will ignore the empty subset.
This is the power set, which we denote as $p(n)$.

$\mbox{LFKNMI}(X, p(n))$ will again be slightly greater than $0.5$. Every cluster in $X$ will
have a perfect match in $p(n)$ and this will result in $H(X|p(n)) = 0$.

In both these examples LFKNMI gives a score slightly above $0.5$. The intuitive behaviour
in these cases would be for a similarity score close to $0$.
We will demonstrate this behaviour in our experiments in \cref{secEval}

When we remove the
normalization from LFKNMI, and instead use a more conventional normalization strategy
\cref{eqnNorms}, we will find more intuitive behaviour.

\section{normalization}
\label{secNormalization}

Typically a normalization will involve a simple division of the absolute quantity
by a quantity which is gauranteed to be an upper bound, giving us a number between
zero and one.

The following sequence of inequalities from \citet{VinhEppsBailey} provide possibilities for normalization.

\begin{equation}
	\begin{split}
	I(X:Y) \leq & \min(H(X),H(Y)) \\
	       \leq & \sqrt{H(X),H(Y)} \\
	       \leq & \frac12 \left(H(X)+H(Y)\right) \\
	       \leq & \max(H(X),H(Y)) \\
	       \leq & H(X,Y)
	\end{split}
\end{equation}

Any of the five expressions on the right can be used, and \cite{VinhEppsBailey} suggest
a measure based on $\max(H(X),H(Y))$. The Normalized Information Distance is recommended
\begin{equation*}
	d_{max} = 1 - \frac{ I(X,Y) }{ \max(H(X),H(Y)) }
\end{equation*}
where zero means perfect similarity and one means dissimilarity. We want a measure with
the opposite behaviour, so we'll use the corresponding normalized mutual information

\begin{equation}
	NMI_{max} = \frac{ I(X:Y) }{ \max(H(X),H(Y)) }
\end{equation}
where $I(X:Y)$ is as defined in \cref{eqnNoComplements,eqnBestMatch,eqnSumVectors,eqnAverageOfTwo}


This can also be understood with reference to \cref{figVenn}. The problem with LFKNMI arises
when one cover is more complicated than the other, for example if one cover has many
more clusters than the other cover.
This corresponds to one circle in \cref{figVenn} being much larger than the other.


\section{evaluation}
\label{secEval}

\begin{figure}
\includegraphics[width=0.5\textwidth]{plot}
\caption{As more communities are found, the score of LFKNMI and $\text{NMI}_{max}$ increase. For a small number of communities found, the intuitive result is a small value, and this is the behaviour of our proposed measure.}
\label{fig1to20}
\end{figure}

See \cref{fig1to20}. There are 200 nodes, divided into 20 communities. Each community has 10 nodes and they do not overlap.
We fix one of our covers, $X$, to be the full set of twenty communities. $Y$ contains a subset of these communities.
As we go from left to right, the number of communities in $Y$ increases from 1 to 20.

The communities in $Y$ are perfect copies of communities in $X$. Therefore, $X=Y$ when all 20 communities are used.
We see this in \cref{fig1to20} at the right, where both measures report an NMI of $1.0$.

This figure confirms the unintuitive behaviour of LFKNMI when few communities are found.
On the left of the figure, when $Y$ has only one community, the score of $0.5$ is unintuitive.

The linear relationship of our NMI$_{max}$, going from 0 to 1 as the number of communities in $Y$ increases, is intuitive.

\section{conclusion}







\break

\section{old}

 \citet{lancichinetti-2009} state the generic \emph{normalized mutual information}
 \begin{equation}
	 I_{norm}(X:Y) = \frac { H(X) + H(Y) - H(X,Y) } { \left( H(X) + H(Y) \right ) /2 }   \label{NMI}
 \end{equation}

They proceed to define a measure based \emph{very loosely} on \cref{VOI}, not on \cref{NMI}. In this note we see how to define a measure
based on \cref{NMI}.

Before considering a normalization, it's important to identify the \emph{absolute} value you wish to consider.
The appropriate measure to consider is
\begin{equation}
	H(X|Y) + H(Y|X)   \label{CorrectMI}
\end{equation}
and, when normalized \footnote{
	It can be shown that \cref{NMI} is equivalent to this, which I believe to be a more understandable formulation:
 \[
	1 - \frac { H(X|Y) + H(Y|X)  }{ H(X) + H(Y) } 
	\]
} is equivalent to \cref{NMI}
This means the amount of information taken to encode X, given that you can refer to Y as a ``codebook'', plus
the information taken to encode Y, given that you can refer to X as a ``codebook''. If $X$ and $Y$ are similar, then
$H(X|Y)$ and $H(Y|X)$ should be close to zero. Then we can identify a suitable normalization (but see the warning in \cref{sec:AbNormal})

\begin{comment}
\section{WARNING}
I'm pretty sloppy, as ever, with my terminology. I just take $H(x)$ to mean the number of bits taken to encode $x$. Hence
I tend to think in $\log_2$, the log to the base 2, when technically I suppose it should be $\log_e$. There's lots of other careless things too, no doubt \begin{turn}{90}(-:\end{turn}.
\end{comment}

\section{problems with LFK's formula}
\label{sec:Flood}

You can get at least 0.5 with LFK's formula by simply flooding the found grouping with every subset of the population.
Assuming $X = \mbox{powerset}(Y)$ is the \emph{found} grouping, this will make the numerator of the first term
very small. But reasonably we expect that the NMI is zero in this case.
%However, this is as much as issue with the definition of $H(X|Y)$ in equation (B.9) of \cite{lancichinetti-2009}.
%More on this after revising the basics of how to encode some information.

Another problem is that is you \emph{find} just one community, and that community is identical to one of the
ground truth communities, then you will also get at least 0.5 for free. This is fundamentally the same problem.

\section{The fixes}
They (loosely) use \cref{VOI} instead of \cref{NMI}. Replacing this fixes the most obvious problem.

The normalization is used in the wrong place.
I have more carefully based my formula on \cref{NMI}, with normalization in the correct place.
I do not yet have a verifiable example of any bizarre result resulted from this issue.

But many of the details I use are taken straight from the LFK formula. ``If it ain't broke, don't fix it.''

\begin{comment}
\section{Basics of encoding}
Given a \grouping, G, made up of $|G|$ groups,
\begin{equation*}
	G = \{ g_1, g_2, \ldots g_{|G|}  \}
\end{equation*}
you can consider G as a matrix of ones and zeros, where each column is vector of $N$ entries recording whether that
node is in that community. A na\"ive encoding scheme is to just set $H(G) = |G| \times N$, which is just one
bit of information for each entry in the matrix. But a more efficient method,
making use of our assumption that community size varies a lot, is to first encode the \emph{size} of the community,
then identify it members. Considering one single community, g,
\begin{equation}
	H(g) = \log_2 N + \log_2 \left( 1/{\binom{N}{|g|}} \right)  \label{OneCommunity}
\end{equation}
where $|g|$ is the number of nodes in group $g$.

This gives us the encoding of an entire (ordered) \grouping,

\[
	H^{*}(G) = \sum_{g \in G} H(g)
\]

But we don't care what order we encode the groups in, so with some handwaving as per the \cite{MOSES}
equivalent grouping argument, we should do
\begin{equation}
	H(G) = \left( \sum_{g \in G} H(g) \right)  - \log_2(|G|!)       \label{SummedG0}
\end{equation}
to get unordered grouping, although this correction mightn't contribute much in many cases. However, I believe that this correction
is important in the case of overlapping communities, as $|G|$ could be very large, especially when
trying to ``cheat'' as per \cref{sec:Flood}.

\section{Encoding with a codebook}
$H(G|F)$ means how many bits does it take to encode G, given that you already have access to F. We'll build it up, one community at a time.
First, the encoding of one community given another (hopefully similar) community, is just the bits taken to encode the difference
between the two communities. I'll define a pseudo-community that is just the nodes that are in $g$, or $f$, but not both, to be $g \Delta f$.
Then
\[
	H(g|f) = H(g \Delta f)
\]
where  $H(g \Delta f)$ is defined as per \cref{OneCommunity} \footnote{
	We probably don't want the complement of the community to be seen as a good match, with low $H(g|f)$, as argued in (B.14) of \cite{lancichinetti-2009}.
	I think this is equivalent to simply requiring that $ |g \Delta f| < N/2 $. And, as they do, define $H(g|f) = H(g)$ if there is no appropriate non-complementary $f$.
}

When you have access to all of $F$, you match $g$ to the best match in $F$, calling if $f^*$. I don't actually use the following
expression, but it's important to illustrate a point.
\begin{equation}
	H(g|F) = \log_2|F| + H(g|f^*)            \label{OneCommFudged}
\end{equation}

The first term, $\log_2|F|$ is left out in (B.9) \cite{lancichinetti-2009}. A term like this is necessary to account for encoding \emph{which} 
found community in the codebook it being used. It is not sufficient to specify which nodes have changed status, one must also identify the community
which is being modified.

So to do the sum in \cref{SummedG0} properly, which requires $H(G,G) = 0$, I'm tempted
\footnote{
	I say ``tempted'' because we're having to fudge something here. Strictly speaking, an efficient encoding,
  which would give us $H(G|G)=0$ would involve allowing each community in $F$ to be selected only once, making
  the first term in \cref{OneCommFudged} a bit smaller, on average, because there would be a smaller set of
	found communities in $F$ to choose from as each g in $G$ in being encoded. So I'm going to cheat a little,
	and fix the ``cheating'' and the $H(G|G)=0$ in another, imperfect, way. The more I think about it though,
	it might even be correct!
}
to do something like this:
\begin{equation}
	H(G|F) = \left( \sum_{g \in G} H(g|f^*) \right)  - \log_2(|G|!) + \log_2(|F|!)       \label{SummedG}
\end{equation}
Here we simply pair up the comms in $G$ and the comms in $F$ by writing them down in two columns and requiring that
each g be paired with one f. Then $\log_2(|F|!)$ is the information needed to present the comms in $F$ in a
suitable order.

Of course, it is unlikely to be the case that $|F|=|G|$, and so we can't really say they're all paired up
with each other. But that doesn't really matter, I think.
This function will do the right thing and be at least as accurate as the existing formulation.

If the \grouping{}s really are identical, then $|F|=|G|$ and the second two terms of \cref{SummedG} will
cancel, and the terms inside the $\sum$ will each be zero as each community will have been lined up
with its best match. If they are very different, then it is important to include these terms as something
is needed to encode which is to match with which.

\section{Bringing it together}
If use use the \emph{normalized mutual information} as per \cref{NMI}, then the log terms
in \cref{SummedG} will cancel out. But they don't cancel in \cref{VOI} and this is a major
reason why the \cref{VOI} can be cheated.

If both are done properly, then I would still have a preference for \cref{NMI}. But maybe then a
case could be made for \cref{VOI}, I don't really know too much about any of this. I have a hunch that, if
\cref{SummedG} is properly used in \cref{VOI} that the two terms are likely to be in the same
ballpark anyway and, also, I suspect that this, corrected, \cref{VOI} would then be similar to \cref{NMI} anyway.


\end{comment}
\section{The cult of normalization}
\label{sec:AbNormal}

The main known problem with the LFK formula arises from aggressive normalization.
One must understand the unnormalized quantity before one can find a reasonable normalization.

When looking at the stability of an algorithm, under something like a random renaming of the nodes, it may be more
appropriate to look at the \emph{absolute} (i.e. un-normalized) amount of information. A good community finding algorithm
will find smaller communities \cite{ResLimit} than does modularity maximization \cite{blondel-2008}, and might
also find overlapping communities. Together this will mean that the found partition, $H(F)$, and presumably the ground truth, $H(G)$, will
each contain a lot of information.

Therefore, it might be that the \emph{absolute} amount of information shared between H(G) and H(F) is high. It would be more
appropriate to use (unnormalized) mutual information
\begin{align*}
	I(X:Y) &=  H(X) + H(Y) - H(X,Y)          \\
				&= \frac12 \left( H(X) + H(Y) - H(X|Y) - H(Y|X) \right)
\end{align*}
If this quantity is larger for one algorithm than another, I'd say this is a better way to define it as being more ``stable''.
An an extreme example, a trivial way to get perfect NMI, and an apparently ``stable'' algorithm, is
to create a \grouping{} with just one community which takes up the whole graph.

If there is a strong desire for some sort of normalization, then that should be a function of the graph, not of either \grouping.
Perhaps divide by $H(\mathrm{the graph})$?

\section{Further modifications}
- As listed in \cite{VinhEppsBailey}: NMI$_{\{joint,max,sqrt,min\}}$. And maybe the Adjusted-for-chance MIs.

- VoI, but without the premature normalization of LFK ?

\section{Experiments to do}
\begin{itemize}
	\item X versus a subset of X. (already done; as expected)
	\item \emph{constant baseline property} independent clusterings should result in 0. Randomly rename nodes? I hypothesize LFK will be noisier.
	\item LFM2: fullCollection versus the first grouping.
	\item Redo the community finding algorithm plots. Will the order of the algorithms change?
\end{itemize}

%\bibliographystyle{IEEEtran}
\bibliographystyle{unsrtnat}
%\bibliographystyle{plain}
\bibliography{community}
\end{document}
